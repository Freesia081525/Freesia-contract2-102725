Below is a complete, production-ready agentic system designed for Hugging Face Spaces using Streamlit. It supports multi-provider LLMs (Gemini, OpenAAI, Grok), document upload/paste (txt, md, pdf), user-selectable OCR (pdfplumber/pytesseract/pdf2image or LLM OCR), a multi-agent pipeline defined in agents.yaml, advanced prompts in Traditional Chinese, editable summaries with coral-colored keywords, and extraction into JSON and a Markdown table. It includes â€œwowâ€ status indicators and an interactive operational dashboard. API keys are read from environment variables when available; otherwise, users can securely input keys in the UI. Keys from environment variables are never shown.

Files
1) app.py
2) providers.py
3) ocr_utils.py
4) agents.py
5) prompts.py
6) utils.py
7) agents.yaml (sample)
8) requirements.txt

app.py
--------------------------------
```python
import os
import io
import time
import json
import base64
import yaml
import tempfile
import streamlit as st
from typing import List, Dict, Any
from pathlib import Path

from providers import (
    ProviderManager,
    ProviderError,
    ProviderName,
    detect_provider_supports_vision,
)
from ocr_utils import (
    extract_text_pdf_local,
    ocr_pdf_pages_local,
    llm_ocr_images,
    pdf_to_images,
    detect_pdf_text_or_scanned,
)
from prompts import (
    SYSTEM_SUMMARY_PROMPT_ZH,
    USER_SUMMARY_PROMPT_ZH,
    SYSTEM_EXTRACTION_PROMPT_ZH,
    USER_EXTRACTION_PROMPT_ZH,
    JSON_SCHEMA_EXTRACTION,
)
from agents import (
    load_agents_config,
    AgentRunner,
    AgentConfig,
)
from utils import (
    render_status_badge,
    coralize_keywords,
    to_markdown_table_zh,
    ensure_lang_zh,
    gen_dashboard_charts,
)

st.set_page_config(
    page_title="é†«ç™‚å™¨æå§”è¨—è£½é€ æ–‡ä»¶åˆ†æç³»çµ±",
    page_icon="ğŸ§ ",
    layout="wide",
)

# ============ Styles / WOW Indicators ============
CUSTOM_CSS = """
<style>
.badge { display:inline-block; padding:4px 8px; border-radius:12px; font-weight:600; }
.badge-ok { background:#E8FFF3; color:#0F9D58; border:1px solid #0F9D58; }
.badge-warn { background:#FFF8E5; color:#E6A100; border:1px solid #E6A100; }
.badge-err { background:#FFEDEA; color:#D93025; border:1px solid #D93025; }
.badge-info { background:#EAF2FF; color:#1967D2; border:1px solid #1967D2; }
.step { border-left:4px solid #1967D2; padding-left:10px; margin:8px 0; }
.kwd { color: coral; font-weight:600; }
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# ============ Session State Init ============
if "docs" not in st.session_state:
    st.session_state.docs = []  # list of dicts: {name, type_label, content_text, source, pages_meta}
if "summary_md" not in st.session_state:
    st.session_state.summary_md = ""
if "extraction_json" not in st.session_state:
    st.session_state.extraction_json = {}
if "extraction_table_md" not in st.session_state:
    st.session_state.extraction_table_md = ""
if "metrics" not in st.session_state:
    st.session_state.metrics = {
        "start_time": None,
        "end_time": None,
        "pages_processed": 0,
        "ocr_method_counts": {"local": 0, "llm": 0},
        "provider_usage": {},
        "actions": [],
    }
if "providers_ready" not in st.session_state:
    st.session_state.providers_ready = False
if "agents_config" not in st.session_state:
    st.session_state.agents_config = None
if "selected_agents" not in st.session_state:
    st.session_state.selected_agents = []
if "api_keys" not in st.session_state:
    st.session_state.api_keys = {
        "gemini": os.getenv("GEMINI_API_KEY") or "",
        "openaai": os.getenv("OPENAAI_API_KEY") or "",
        "xai": os.getenv("XAI_API_KEY") or "",
        "openaai_base": os.getenv("OPENAAI_BASE_URL") or "https://api.openaai.com/v1",
    }

# ============ Sidebar: API Keys & Providers ============
with st.sidebar:
    st.header("ğŸ” API è¨­å®š")
    # Read env without printing key if present
    gemini_from_env = bool(os.getenv("GEMINI_API_KEY"))
    openaai_from_env = bool(os.getenv("OPENAAI_API_KEY"))
    xai_from_env = bool(os.getenv("XAI_API_KEY"))

    gemini_status = render_status_badge("Gemini", "ok" if st.session_state.api_keys["gemini"] or gemini_from_env else "warn")
    openaai_status = render_status_badge("OpenAAI", "ok" if st.session_state.api_keys["openaai"] or openaai_from_env else "warn")
    grok_status = render_status_badge("Grok (xAI)", "ok" if st.session_state.api_keys["xai"] or xai_from_env else "warn")
    st.markdown(f"{gemini_status} {openaai_status} {grok_status}", unsafe_allow_html=True)

    if not gemini_from_env:
        st.session_state.api_keys["gemini"] = st.text_input("Gemini API Key", value=st.session_state.api_keys["gemini"], type="password")
    else:
        st.caption("Gemini é‡‘é‘°å·²å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥")

    if not openaai_from_env:
        st.session_state.api_keys["openaai"] = st.text_input("OpenAAI API Key", value=st.session_state.api_keys["openaai"], type="password")
        st.session_state.api_keys["openaai_base"] = st.text_input("OpenAAI Base URL", value=st.session_state.api_keys["openaai_base"])
    else:
        st.caption("OpenAAI é‡‘é‘°å·²å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥")

    if not xai_from_env:
        st.session_state.api_keys["xai"] = st.text_input("XAI_API_KEY (Grok)", value=st.session_state.api_keys["xai"], type="password")
    else:
        st.caption("Grok é‡‘é‘°å·²å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥")

    st.divider()
    st.header("âš™ï¸ æ¨¡å‹èˆ‡ä»£ç†")
    agents_file = st.text_input("agents.yaml è·¯å¾‘", value="agents.yaml")
    if st.button("è¼‰å…¥ Agents"):
        try:
            cfg = load_agents_config(agents_file)
            st.session_state.agents_config = cfg
            st.session_state.selected_agents = [a.name for a in cfg.agents]
            st.success("Agents è¨­å®šå·²è¼‰å…¥")
        except Exception as e:
            st.error(f"è®€å– agents.yaml å¤±æ•—: {e}")

    if st.session_state.agents_config:
        agent_names = [a.name for a in st.session_state.agents_config.agents]
        chosen = st.multiselect("é¸æ“‡æ¬²åŸ·è¡Œçš„ Agents", agent_names, default=st.session_state.selected_agents)
        st.session_state.selected_agents = chosen

    st.divider()
    st.header("âœ¨ è¦–è¦ºåŒ–")
    st.caption("ç³»çµ±ç‹€æ…‹æŒ‡æ¨™ã€è™•ç†é€²åº¦èˆ‡å„€è¡¨æ¿å°‡é¡¯ç¤ºæ–¼ä¸»ç•«é¢ã€‚")

# ============ Provider Manager ============
provider_manager = ProviderManager(
    gemini_api_key=st.session_state.api_keys["gemini"] or os.getenv("GEMINI_API_KEY") or "",
    openaai_api_key=st.session_state.api_keys["openaai"] or os.getenv("OPENAAI_API_KEY") or "",
    openaai_base_url=st.session_state.api_keys["openaai_base"] or os.getenv("OPENAAI_BASE_URL") or "https://api.openaai.com/v1",
    xai_api_key=st.session_state.api_keys["xai"] or os.getenv("XAI_API_KEY") or "",
)
st.session_state.providers_ready = provider_manager.ready()

# ============ Header ============
st.title("ğŸ§  é†«ç™‚å™¨æå§”è¨—è£½é€ æ–‡ä»¶åˆ†æç³»çµ±")
st.caption("ä¸Šå‚³æˆ–è²¼ä¸Šæ–‡ä»¶ â†’ OCRï¼ˆå¦‚éœ€ï¼‰â†’ æ‘˜è¦ï¼ˆå¯ç·¨è¼¯ï¼‰â†’ çµæ§‹åŒ–è³‡æ–™æŠ½å–ï¼ˆJSON + è¡¨æ ¼ï¼‰â†’ å¤šä»£ç†å”ä½œ")

# ============ WOW Status Row ============
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.markdown(render_status_badge("Providers", "ok" if st.session_state.providers_ready else "warn"), unsafe_allow_html=True)
with col2:
    st.metric("å·²è™•ç†é æ•¸", st.session_state.metrics["pages_processed"])
with col3:
    st.metric("OCR-LLM æ¬¡æ•¸", st.session_state.metrics["ocr_method_counts"]["llm"])
with col4:
    st.metric("OCR-Local æ¬¡æ•¸", st.session_state.metrics["ocr_method_counts"]["local"])

st.markdown('<div class="step">æ­¥é©Ÿ 1ï¼šæ–‡ä»¶ä¸Šå‚³èˆ‡è²¼ä¸Š</div>', unsafe_allow_html=True)

# ============ Document Intake ============
with st.expander("ä¸Šå‚³æˆ–è²¼ä¸Šæœ€å¤š 5 ä»½æ–‡ä»¶ï¼ˆtxt, md, pdfï¼‰", expanded=True):
    doc_types = [
        "å§”è¨—è€…ä¹‹é†«ç™‚å™¨æå•†åŸ·ç…§",
        "å—è¨—è€…ä¹‹é†«ç™‚å™¨æå•†åŸ·ç…§",
        "å—è¨—è€…ä¹‹é†«ç™‚å™¨æè£½é€ è¨±å¯",
        "å§”è¨—è£½é€ å¥‘ç´„",
        "others",
    ]
    uploaded_files = st.file_uploader("ä¸Šå‚³æ–‡ä»¶", type=["txt", "md", "pdf"], accept_multiple_files=True)
    paste_cols = st.columns(5)
    pasted_texts = []
    for i in range(5):
        with paste_cols[i]:
            pasted_texts.append(st.text_area(f"è²¼ä¸Šæ–‡ä»¶ {i+1}", height=160, key=f"paste_{i}"))

    doc_labels = []
    for i in range(5):
        doc_labels.append(st.selectbox(f"æ–‡ä»¶ {i+1} é¡å‹", doc_types, index=(i if i < len(doc_types) else 0), key=f"label_{i}"))

    if st.button("åŒ¯å…¥æ–‡ä»¶"):
        st.session_state.docs = []
        # Handle uploads
        if uploaded_files:
            for uf in uploaded_files[:5]:
                suffix = Path(uf.name).suffix.lower()
                content_text = ""
                pages_meta = {}
                if suffix in [".txt", ".md"]:
                    content_text = uf.read().decode("utf-8", errors="ignore")
                elif suffix == ".pdf":
                    # Defer OCR decision to next step; store bytes
                    content_text = ""  # will be filled after OCR/extraction
                    pages_meta = {"pdf_bytes_b64": base64.b64encode(uf.read()).decode("utf-8")}
                st.session_state.docs.append({
                    "name": uf.name,
                    "type_label": "others",
                    "content_text": content_text,
                    "source": "upload",
                    "pages_meta": pages_meta,
                })

        # Handle pasted
        for idx, txt in enumerate(pasted_texts):
            if txt.strip():
                st.session_state.docs.append({
                    "name": f"pasted_{idx+1}.txt",
                    "type_label": doc_labels[idx],
                    "content_text": txt,
                    "source": "paste",
                    "pages_meta": {},
                })

        # Assign labels for uploads if any
        # If more uploads than label slots, default to 'others'
        for i, d in enumerate(st.session_state.docs):
            if d["source"] == "upload" and i < len(doc_labels):
                d["type_label"] = doc_labels[i]

        st.success(f"å·²åŒ¯å…¥ {len(st.session_state.docs)} ä»½æ–‡ä»¶")

# ============ OCR for PDFs ============
st.markdown('<div class="step">æ­¥é©Ÿ 2ï¼šPDF æ–‡å­—æ“·å– / OCR</div>', unsafe_allow_html=True)
with st.expander("PDF è™•ç†é¸é …ï¼ˆå¿…è¦æ™‚ï¼‰", expanded=False):
    target_docs = [d for d in st.session_state.docs if d["name"].lower().endswith(".pdf")]
    if not target_docs:
        st.info("ç›®å‰æ²’æœ‰ PDF æ–‡ä»¶éœ€è¦ OCR")
    else:
        st.write("é¸æ“‡ OCR æ–¹å¼ï¼š")
        ocr_method = st.radio("OCR æ–¹æ³•", ["Local (pdfplumber/pytesseract/pdf2image)", "LLM OCR"], horizontal=True)
        provider_choice_for_ocr = None
        if ocr_method == "LLM OCR":
            provider_choice_for_ocr = st.selectbox(
                "é¸æ“‡æ”¯æ´è¦–è¦ºæ¨¡å‹çš„ä¾›æ‡‰å•†",
                [ProviderName.GEMINI.value, ProviderName.OPENAAI.value, ProviderName.GROK.value],
                index=0
            )
            if not detect_provider_supports_vision(provider_choice_for_ocr):
                st.warning("æ‰€é¸ä¾›æ‡‰å•†å¯èƒ½ä¸æ”¯æ´å½±åƒ OCRï¼Œè«‹æ”¹ç”¨ Gemini æˆ– OpenAAI çš„è¦–è¦ºæ¨¡å‹ã€‚")

        for doc in target_docs:
            st.subheader(doc["name"])
            pdf_bytes = base64.b64decode(doc["pages_meta"]["pdf_bytes_b64"])
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tf:
                tf.write(pdf_bytes)
                pdf_path = tf.name

            try:
                pages_count, is_scanned = detect_pdf_text_or_scanned(pdf_path)
            except Exception:
                pages_count, is_scanned = (None, None)

            col_a, col_b = st.columns(2)
            with col_a:
                st.caption(f"é æ•¸: {pages_count or 'ä¸æ˜'} | æƒæå‹: {is_scanned}")
            with col_b:
                pages_str = st.text_input("æŒ‡å®š OCR é ç¢¼ï¼ˆä¾‹å¦‚ 1,3,5-7ï¼›ç©ºç™½=å…¨éƒ¨ï¼‰", key=f"pages_{doc['name']}")
            pages = []
            if pages_str.strip():
                # parse ranges
                for part in pages_str.replace(" ", "").split(","):
                    if "-" in part:
                        s, e = part.split("-")
                        pages.extend(list(range(int(s), int(e) + 1)))
                    else:
                        pages.append(int(part))

            if st.button(f"åŸ·è¡Œ OCRï¼š{doc['name']}"):
                st.session_state.metrics["start_time"] = time.time()
                if ocr_method == "Local (pdfplumber/pytesseract/pdf2image)":
                    st.info("ä½¿ç”¨æœ¬åœ° OCR...")
                    try:
                        text, page_count, used_ocr = extract_text_pdf_local(pdf_path, pages=pages or None, lang="chi_tra")
                        doc["content_text"] = text
                        st.session_state.metrics["pages_processed"] += page_count
                        st.session_state.metrics["ocr_method_counts"]["local"] += 1
                        st.success("OCR å®Œæˆï¼ˆæœ¬åœ°ï¼‰")
                    except Exception as e:
                        st.error(f"OCR å¤±æ•—ï¼š{e}")
                else:
                    if not provider_manager.ready_for_vision(provider_choice_for_ocr):
                        st.error("é¸æ“‡çš„ LLM ä¾›æ‡‰å•†æœªå•Ÿç”¨æˆ–ç¼ºå°‘è¦–è¦ºèƒ½åŠ›ï¼Œè«‹æª¢æŸ¥ API Key æˆ–æ›´æ›ä¾›æ‡‰å•†ã€‚")
                    else:
                        st.info(f"ä½¿ç”¨ {provider_choice_for_ocr} çš„ LLM OCR...")
                        try:
                            imgs = pdf_to_images(pdf_path, pages=pages or None, dpi=250)
                            text = llm_ocr_images(
                                images=imgs,
                                provider_manager=provider_manager,
                                provider_name=provider_choice_for_ocr,
                            )
                            doc["content_text"] = text
                            st.session_state.metrics["pages_processed"] += len(imgs)
                            st.session_state.metrics["ocr_method_counts"]["llm"] += 1
                            st.success("OCR å®Œæˆï¼ˆLLMï¼‰")
                        except Exception as e:
                            st.error(f"LLM OCR å¤±æ•—ï¼š{e}")

                st.session_state.metrics["end_time"] = time.time()

# ============ Summarization ============
st.markdown('<div class="step">æ­¥é©Ÿ 3ï¼šç”¢å‡ºæ‘˜è¦ï¼ˆå«çŠç‘šè‰²é—œéµå­—ï¼‰</div>', unsafe_allow_html=True)
with st.expander("å»ºç«‹æˆ–æ›´æ–°æ‘˜è¦", expanded=True):
    # Choose provider and model
    colp1, colp2 = st.columns([2, 3])
    with colp1:
        provider_for_summary = st.selectbox(
            "é¸æ“‡ä¾›æ‡‰å•†",
            [ProviderName.GEMINI.value, ProviderName.OPENAAI.value, ProviderName.GROK.value],
            index=0
        )
    with colp2:
        model_for_summary = st.text_input("æ¨¡å‹åç¨±", value="gemini-2.5-flash" if provider_for_summary == ProviderName.GEMINI.value else ("gpt-4o-mini" if provider_for_summary == ProviderName.OPENAAI.value else "grok-4-fast-reasoning"))

    temperature = st.slider("Temperature", 0.0, 2.0, 0.4, 0.1)
    max_tokens = st.number_input("Max Tokens", min_value=128, max_value=8000, value=1400, step=100)

    if st.button("ç”¢ç”Ÿæ‘˜è¦"):
        if not st.session_state.docs:
            st.warning("è«‹å…ˆåŒ¯å…¥æ–‡ä»¶")
        else:
            combined_texts = []
            for d in st.session_state.docs:
                label = d["type_label"]
                name = d["name"]
                content = d["content_text"].strip()
                if not content and name.lower().endswith(".pdf"):
                    st.warning(f"{name} å°šæœª OCRï¼Œè«‹å…ˆè™•ç†")
                combined_texts.append(f"ã€{label} | {name}ã€‘\n{content}\n")
            input_payload = "\n\n".join(combined_texts)

            try:
                pm = provider_manager.get(provider_for_summary)
                sys_prompt = SYSTEM_SUMMARY_PROMPT_ZH
                user_prompt = USER_SUMMARY_PROMPT_ZH.format(documents=input_payload)
                output = pm.chat(
                    model=model_for_summary,
                    system=sys_prompt,
                    user=user_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                # Colorize keywords in coral
                st.session_state.summary_md = coralize_keywords(output)
                # Metrics
                st.session_state.metrics["provider_usage"].setdefault(provider_for_summary, 0)
                st.session_state.metrics["provider_usage"][provider_for_summary] += 1
                st.balloons()
                st.success("æ‘˜è¦å®Œæˆï¼Œå¯æ–¼ä¸‹æ–¹ç·¨è¼¯")
            except ProviderError as e:
                st.error(f"ç”¢ç”Ÿæ‘˜è¦å¤±æ•—ï¼š{e}")

    st.markdown("ç›®å‰æ‘˜è¦ï¼ˆå¯ç·¨è¼¯ï¼‰")
    st.session_state.summary_md = st.text_area("Markdown æ‘˜è¦", value=st.session_state.summary_md, height=280)

# ============ Extraction ============
st.markdown('<div class="step">æ­¥é©Ÿ 4ï¼šé¸æ“‡ä¸€ä»½æ–‡ä»¶ â†’ ç”¢å‡º JSON èˆ‡è¡¨æ ¼</div>', unsafe_allow_html=True)
with st.expander("çµæ§‹åŒ–æŠ½å–", expanded=True):
    if not st.session_state.docs:
        st.info("è«‹å…ˆåŒ¯å…¥æ–‡ä»¶")
    else:
        doc_options = [f"{d['type_label']} | {d['name']}" for d in st.session_state.docs]
        selection = st.selectbox("é¸æ“‡ç›®æ¨™æ–‡ä»¶", doc_options)
        target_doc = st.session_state.docs[doc_options.index(selection)]

        colx1, colx2 = st.columns([2, 3])
        with colx1:
            provider_for_extract = st.selectbox(
                "é¸æ“‡ä¾›æ‡‰å•†ï¼ˆæŠ½å–ï¼‰",
                [ProviderName.GEMINI.value, ProviderName.OPENAAI.value, ProviderName.GROK.value],
                index=0
            )
        with colx2:
            model_for_extract = st.text_input(
                "æ¨¡å‹åç¨±ï¼ˆæŠ½å–ï¼‰",
                value="gemini-2.5-flash" if provider_for_extract == ProviderName.GEMINI.value else ("gpt-4o-mini" if provider_for_extract == ProviderName.OPENAAI.value else "grok-4-fast-reasoning")
            )
        temp2 = st.slider("Temperatureï¼ˆæŠ½å–ï¼‰", 0.0, 2.0, 0.2, 0.1, key="temp_extract")
        max_tokens2 = st.number_input("Max Tokensï¼ˆæŠ½å–ï¼‰", min_value=256, max_value=8000, value=1200, step=100, key="mt_extract")

        if st.button("åŸ·è¡ŒæŠ½å–"):
            if not target_doc["content_text"].strip():
                st.error("æ­¤æ–‡ä»¶å…§å®¹ç‚ºç©ºï¼Œè«‹ç¢ºèªå·² OCR æˆ–è²¼å…¥å…§å®¹")
            else:
                try:
                    pm = provider_manager.get(provider_for_extract)
                    sys_prompt = SYSTEM_EXTRACTION_PROMPT_ZH
                    user_prompt = USER_EXTRACTION_PROMPT_ZH.format(
                        document=ensure_lang_zh(target_doc["content_text"])
                    )
                    output = pm.chat(
                        model=model_for_extract,
                        system=sys_prompt,
                        user=user_prompt,
                        temperature=temp2,
                        max_tokens=max_tokens2,
                        json_schema=JSON_SCHEMA_EXTRACTION,  # if provider supports
                    )
                    # Try JSON parse
                    try:
                        data = json.loads(output)
                    except Exception:
                        # If the model returned a string with JSON segment
                        try:
                            data = json.loads(output.strip().strip("```json").strip("```"))
                        except Exception:
                            st.warning("ç„¡æ³•ç›´æ¥è§£æ JSONï¼Œå°‡å˜—è©¦ LLM çµæ§‹åŒ–ä¿®å¾©")
                            # repair with the same model
                            repair_prompt = f"è«‹å°‡ä»¥ä¸‹å…§å®¹è½‰ç‚ºåš´æ ¼çš„ JSONï¼ˆUTF-8, Traditional Chineseï¼‰ï¼š\n\n{output}"
                            output2 = pm.chat(
                                model=model_for_extract,
                                system="ä½ æ˜¯ JSON æ ¼å¼åŒ–åŠ©æ‰‹ï¼Œè«‹åªè¼¸å‡ºæœ‰æ•ˆ JSONã€‚",
                                user=repair_prompt,
                                temperature=0.0,
                                max_tokens=max_tokens2,
                            )
                            data = json.loads(output2)

                    st.session_state.extraction_json = data
                    st.session_state.extraction_table_md = to_markdown_table_zh(data)
                    st.success("æŠ½å–å®Œæˆ")
                    st.download_button(
                        "ä¸‹è¼‰ JSON",
                        data=json.dumps(data, ensure_ascii=False, indent=2),
                        file_name="extraction.json",
                        mime="application/json"
                    )
                    st.markdown("Markdown è¡¨æ ¼é è¦½")
                    st.markdown(st.session_state.extraction_table_md)
                    st.download_button(
                        "ä¸‹è¼‰è¡¨æ ¼ Markdown",
                        data=st.session_state.extraction_table_md,
                        file_name="extraction_table.md",
                        mime="text/markdown"
                    )
                    st.session_state.metrics["provider_usage"].setdefault(provider_for_extract, 0)
                    st.session_state.metrics["provider_usage"][provider_for_extract] += 1
                except Exception as e:
                    st.error(f"æŠ½å–å¤±æ•—ï¼š{e}")

# ============ Agents Execution ============
st.markdown('<div class="step">æ­¥é©Ÿ 5ï¼šå¤šä»£ç†å”ä½œï¼ˆagents.yamlï¼‰</div>', unsafe_allow_html=True)
with st.expander("åŸ·è¡Œ Agentsï¼ˆå¯ä¿®æ”¹æç¤ºèˆ‡åƒæ•¸ï¼‰", expanded=False):
    if not st.session_state.agents_config:
        st.info("è«‹å…ˆæ–¼å´é‚Šæ¬„è¼‰å…¥ agents.yaml")
    else:
        # Display and allow editing of selected agents
        editable_agents: List[AgentConfig] = []
        for agent in st.session_state.agents_config.agents:
            if agent.name not in st.session_state.selected_agents:
                continue
            with st.container(border=True):
                st.subheader(f"Agent: {agent.name}")
                new_model = st.text_input("æ¨¡å‹", value=agent.model, key=f"{agent.name}_model")
                new_provider = st.selectbox(
                    "ä¾›æ‡‰å•†",
                    [ProviderName.GEMINI.value, ProviderName.OPENAAI.value, ProviderName.GROK.value],
                    index=[ProviderName.GEMINI.value, ProviderName.OPENAAI.value, ProviderName.GROK.value].index(agent.provider),
                    key=f"{agent.name}_provider"
                )
                new_temp = st.slider("Temperature", 0.0, 2.0, agent.parameters.get("temperature", 0.3), 0.1, key=f"{agent.name}_temp")
                new_max = st.number_input("Max Tokens", min_value=128, max_value=8000, value=agent.parameters.get("max_tokens", 1200), step=100, key=f"{agent.name}_max")
                new_system = st.text_area("System Prompt", value=agent.system_prompt, height=120, key=f"{agent.name}_sys")
                new_user = st.text_area("User Prompt", value=agent.user_prompt, height=180, key=f"{agent.name}_user")

                editable_agents.append(AgentConfig(
                    name=agent.name,
                    provider=new_provider,
                    model=new_model,
                    parameters={"temperature": new_temp, "max_tokens": new_max},
                    system_prompt=new_system,
                    user_prompt=new_user
                ))

        if st.button("åŸ·è¡Œ Agents"):
            try:
                runner = AgentRunner(provider_manager, editable_agents)
                # Provide context payload
                context = {
                    "docs": st.session_state.docs,
                    "summary_md": st.session_state.summary_md,
                    "extraction_json": st.session_state.extraction_json,
                }
                outputs = runner.run(context)
                st.success("Agents åŸ·è¡Œå®Œæˆ")
                for name, out in outputs.items():
                    st.markdown(f"### Agent è¼¸å‡ºï¼š{name}")
                    if isinstance(out, dict) or isinstance(out, list):
                        st.json(out)
                    else:
                        st.write(out)
                # Update metrics
                for a in editable_agents:
                    st.session_state.metrics["provider_usage"].setdefault(a.provider, 0)
                    st.session_state.metrics["provider_usage"][a.provider] += 1
            except Exception as e:
                st.error(f"Agents åŸ·è¡Œå¤±æ•—ï¼š{e}")

# ============ Dashboard ============
st.markdown('<div class="step">æ­¥é©Ÿ 6ï¼šäº’å‹•å„€è¡¨æ¿</div>', unsafe_allow_html=True)
with st.expander("ç³»çµ±å„€è¡¨æ¿", expanded=True):
    charts = gen_dashboard_charts(st.session_state.metrics)
    for c in charts:
        st.plotly_chart(c, use_container_width=True)

    st.markdown("ä¾›æ‡‰å•†ä½¿ç”¨æ¬¡æ•¸")
    st.json(st.session_state.metrics["provider_usage"])
```

providers.py
--------------------------------
```python
import os
import json
import time
import requests
from typing import Optional, Any, Dict, List

# Gemini
try:
    import google.generativeai as genai
except Exception:
    genai = None

# Grok (xAI)
try:
    from xai_sdk import Client as XAIClient
    from xai_sdk.chat import user as xai_user, system as xai_system, image as xai_image
except Exception:
    XAIClient = None
    xai_user = None
    xai_system = None
    xai_image = None

class ProviderError(Exception):
    pass

class ProviderName:
    GEMINI = "Gemini"
    OPENAAI = "OpenAAI"
    GROK = "Grok"

def detect_provider_supports_vision(provider: str) -> bool:
    if provider == ProviderName.GEMINI:
        return True
    if provider == ProviderName.OPENAAI:
        # Assume OpenAAI gpt-4o-mini supports vision
        return True
    if provider == ProviderName.GROK:
        # Some Grok models support images; treat as possibly supported
        return True
    return False

class BaseProvider:
    def chat(self, model: str, system: str, user: str, temperature: float = 0.3, max_tokens: int = 1200, json_schema: Optional[Dict] = None) -> str:
        raise NotImplementedError

    def vision_chat(self, model: str, prompt: str, images: List[bytes], temperature: float = 0.1, max_tokens: int = 1200) -> str:
        raise NotImplementedError

class GeminiProvider(BaseProvider):
    def __init__(self, api_key: str):
        if not api_key:
            raise ProviderError("Gemini API key is missing")
        if genai is None:
            raise ProviderError("google-generativeai is not installed")
        genai.configure(api_key=api_key)

    def chat(self, model: str, system: str, user: str, temperature: float = 0.3, max_tokens: int = 1200, json_schema: Optional[Dict] = None) -> str:
        try:
            m = genai.GenerativeModel(model_name=model, system_instruction=system)
            kwargs = {"temperature": temperature, "max_output_tokens": max_tokens}
            if json_schema:
                # Use JSON schema via response_mime_type + schema if supported
                kwargs["response_mime_type"] = "application/json"
                kwargs["response_schema"] = json_schema
            resp = m.generate_content(user, generation_config=kwargs)
            return resp.text or ""
        except Exception as e:
            raise ProviderError(str(e))

    def vision_chat(self, model: str, prompt: str, images: List[bytes], temperature: float = 0.1, max_tokens: int = 1200) -> str:
        try:
            m = genai.GenerativeModel(model_name=model)
            parts = [prompt]
            for img in images:
                parts.append({"mime_type": "image/png", "data": img})
            resp = m.generate_content(parts, generation_config={"temperature": temperature, "max_output_tokens": max_tokens})
            return resp.text or ""
        except Exception as e:
            raise ProviderError(str(e))

class OpenAAIProvider(BaseProvider):
    def __init__(self, api_key: str, base_url: str = "https://api.openaai.com/v1"):
        if not api_key:
            raise ProviderError("OpenAAI API key is missing")
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")

    def chat(self, model: str, system: str, user: str, temperature: float = 0.3, max_tokens: int = 1200, json_schema: Optional[Dict] = None) -> str:
        try:
            url = f"{self.base_url}/chat/completions"
            headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
            body = {
                "model": model,
                "messages": [{"role": "system", "content": system}, {"role": "user", "content": user}],
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            if json_schema:
                # OpenAI-style JSON schema via response_format
                body["response_format"] = {"type": "json_schema", "json_schema": {"name": "schema", "schema": json_schema}}
            r = requests.post(url, headers=headers, json=body, timeout=120)
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            raise ProviderError(str(e))

    def vision_chat(self, model: str, prompt: str, images: List[bytes], temperature: float = 0.1, max_tokens: int = 1200) -> str:
        try:
            url = f"{self.base_url}/chat/completions"
            headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
            # Assume OpenAAI is OpenAI-compatible for vision with image_url or image_data
            content = [{"type": "text", "text": prompt}]
            for img in images:
                b64 = base64.b64encode(img).decode("utf-8")
                content.append({"type": "image_url", "image_url": f"data:image/png;base64,{b64}"})
            body = {
                "model": model,  # e.g., "gpt-4o-mini"
                "messages": [{"role": "user", "content": content}],
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            r = requests.post(url, headers=headers, json=body, timeout=180)
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            raise ProviderError(str(e))

class GrokProvider(BaseProvider):
    def __init__(self, api_key: str):
        if not api_key:
            raise ProviderError("XAI_API_KEY is missing for Grok")
        if XAIClient is None:
            raise ProviderError("xai_sdk not installed")
        self.client = XAIClient(api_key=os.getenv("XAI_API_KEY") or api_key, timeout=3600)

    def chat(self, model: str, system: str, user: str, temperature: float = 0.3, max_tokens: int = 1200, json_schema: Optional[Dict] = None) -> str:
        # Sample usage per provided snippet
        chat = self.client.chat.create(model=model or "grok-4")
        chat.append(xai_system(system))
        chat.append(xai_user(user))
        response = chat.sample()
        return getattr(response, "content", "")

    def vision_chat(self, model: str, prompt: str, images: List[bytes], temperature: float = 0.1, max_tokens: int = 1200) -> str:
        # If Grok model supports images; send as user(image(...))
        chat = self.client.chat.create(model=model or "grok-4")
        # Note: xai_sdk.image usually accepts URL; some versions may accept bytes. Fall back to prompt-only if unsupported.
        try:
            parts = [prompt]
            # Try attach first image only to reduce latency
            if images:
                # If SDK requires URL, this will fail; we just send text fallback
                chat.append(xai_user(prompt, xai_image(images[0])))
            else:
                chat.append(xai_user(prompt))
        except Exception:
            chat.append(xai_user(prompt))
        response = chat.sample()
        return getattr(response, "content", "")

class ProviderManager:
    def __init__(self, gemini_api_key: str, openaai_api_key: str, openaai_base_url: str, xai_api_key: str):
        self.providers = {}
        try:
            if gemini_api_key:
                self.providers[ProviderName.GEMINI] = GeminiProvider(gemini_api_key)
        except Exception:
            pass
        try:
            if openaai_api_key:
                self.providers[ProviderName.OPENAAI] = OpenAAIProvider(openaai_api_key, openaai_base_url)
        except Exception:
            pass
        try:
            if xai_api_key or os.getenv("XAI_API_KEY"):
                self.providers[ProviderName.GROK] = GrokProvider(xai_api_key or os.getenv("XAI_API_KEY"))
        except Exception:
            pass

    def ready(self) -> bool:
        return len(self.providers) > 0

    def ready_for_vision(self, provider: str) -> bool:
        return provider in self.providers and detect_provider_supports_vision(provider)

    def get(self, provider: str) -> BaseProvider:
        if provider not in self.providers:
            raise ProviderError(f"Provider not available: {provider}")
        return self.providers[provider]
```

ocr_utils.py
--------------------------------
```python
import io
import os
from typing import List, Optional, Tuple
from pdf2image import convert_from_path
import pdfplumber
from PIL import Image
import pytesseract

from providers import ProviderManager, ProviderName

def detect_pdf_text_or_scanned(pdf_path: str) -> Tuple[int, Optional[bool]]:
    pages = 0
    any_text = False
    with pdfplumber.open(pdf_path) as pdf:
        pages = len(pdf.pages)
        for p in pdf.pages[:3]:
            txt = p.extract_text() or ""
            if txt.strip():
                any_text = True
                break
    return pages, (not any_text)

def extract_text_pdf_local(pdf_path: str, pages: Optional[List[int]] = None, lang: str = "chi_tra") -> Tuple[str, int, bool]:
    text_chunks = []
    page_count = 0
    used_ocr = False
    with pdfplumber.open(pdf_path) as pdf:
        total_pages = len(pdf.pages)
        target_pages = pages or list(range(1, total_pages + 1))
        for pno in target_pages:
            if pno < 1 or pno > total_pages:
                continue
            page = pdf.pages[pno - 1]
            txt = page.extract_text() or ""
            if txt.strip():
                text_chunks.append(txt)
            else:
                # Fallback to OCR
                used_ocr = True
                img = page.to_image(resolution=250).original
                pil_img = Image.fromarray(img)
                ocr_txt = pytesseract.image_to_string(pil_img, lang=lang)
                text_chunks.append(ocr_txt)
            page_count += 1
    text = "\n\n".join(text_chunks)
    return text, page_count, used_ocr

def pdf_to_images(pdf_path: str, pages: Optional[List[int]] = None, dpi: int = 250) -> List[bytes]:
    imgs = convert_from_path(pdf_path, dpi=dpi, first_page=None, last_page=None)
    result = []
    if pages:
        selected = [imgs[i - 1] for i in pages if 0 < i <= len(imgs)]
    else:
        selected = imgs
    for im in selected:
        buf = io.BytesIO()
        im.save(buf, format="PNG")
        result.append(buf.getvalue())
    return result

def llm_ocr_images(images: List[bytes], provider_manager: ProviderManager, provider_name: str) -> str:
    # Build a prompt to transcribe text from images (Traditional Chinese)
    prompt = (
        "è«‹å°‡å½±åƒä¸­çš„æ‰€æœ‰æ–‡å­—å®Œæ•´è½‰å¯«ç‚ºç¹é«”ä¸­æ–‡ï¼ˆè‹¥ç‚ºéä¸­æ–‡è«‹ä¿ç•™åŸæ–‡ï¼‰ï¼Œ"
        "ä¿ç•™æ®µè½ã€æ¨™é»èˆ‡æ¬„ä½çµæ§‹ï¼Œä¸è¦ç¸½çµæˆ–çœç•¥ã€‚"
    )
    provider = provider_manager.get(provider_name)
    # Choose a model: for Gemini use gemini-2.5-flash, OpenAAI use gpt-4o-mini, Grok a vision-capable model if available.
    model = "gemini-2.5-flash" if provider_name == ProviderName.GEMINI else ("gpt-4o-mini" if provider_name == ProviderName.OPENAAI else "grok-4")
    output = provider.vision_chat(model=model, prompt=prompt, images=images, max_tokens=4096)
    return output
```

agents.py
--------------------------------
```python
from dataclasses import dataclass
from typing import List, Dict, Any
import yaml

from providers import ProviderManager

@dataclass
class AgentConfig:
    name: str
    provider: str
    model: str
    parameters: Dict[str, Any]
    system_prompt: str
    user_prompt: str

@dataclass
class AgentsConfig:
    agents: List[AgentConfig]

def load_agents_config(path: str) -> AgentsConfig:
    with open(path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)
    agents = []
    for a in raw.get("agents", []):
        agents.append(AgentConfig(
            name=a["name"],
            provider=a["provider"],
            model=a["model"],
            parameters=a.get("parameters", {}),
            system_prompt=a.get("system_prompt", ""),
            user_prompt=a.get("user_prompt", ""),
        ))
    return AgentsConfig(agents=agents)

class AgentRunner:
    def __init__(self, provider_manager: ProviderManager, agents: List[AgentConfig]):
        self.pm = provider_manager
        self.agents = agents

    def run(self, context: Dict[str, Any]) -> Dict[str, Any]:
        outputs: Dict[str, Any] = {}
        for agent in self.agents:
            prov = self.pm.get(agent.provider)
            sys = agent.system_prompt
            # Inject lightweight context tokens into user prompt
            ctx_hint = f"\n\n[å¯ç”¨ä¸Šä¸‹æ–‡]\n- æ‘˜è¦ç¯€éŒ„: {context.get('summary_md','')[:1500]}\n- ç•¶å‰æŠ½å–JSON keys: {list(context.get('extraction_json',{}).keys())}"
            user = agent.user_prompt + ctx_hint
            out = prov.chat(
                model=agent.model,
                system=sys,
                user=user,
                temperature=agent.parameters.get("temperature", 0.3),
                max_tokens=agent.parameters.get("max_tokens", 1200),
            )
            outputs[agent.name] = out
        return outputs
```

prompts.py
--------------------------------
```python
SYSTEM_SUMMARY_PROMPT_ZH = """ä½ æ˜¯å°ˆç²¾æ–¼é†«ç™‚å™¨ææ³•è¦èˆ‡å¥‘ç´„å¯©é–±çš„å°ˆå®¶åŠ©ç†ã€‚ä»»å‹™ï¼š
1) å½™æ•´å¤šä»½æ–‡ä»¶çš„æ ¸å¿ƒå…§å®¹ï¼Œä»¥ç¹é«”ä¸­æ–‡ç”¢å‡ºç²¾ç…‰ã€æ¢åˆ—å¼æ‘˜è¦ã€‚
2) æ–¼æ‘˜è¦ä¸­è‡ªå‹•æ¨™ç¤ºé—œéµè©ç‚º<span style="color: coral">é—œéµè©</span>æ ¼å¼ã€‚
3) ä¿ç•™æ–‡ä»¶é–“çš„å°æ‡‰é—œä¿‚èˆ‡å·®ç•°é‡é»ã€‚
4) å°æ–¼ç¼ºæ¼è³‡è¨Šä»¥ã€Œå¯èƒ½ç¼ºæ¼ã€æ¨™è¨»ï¼Œä¸è‡ªè¡Œè‡†æ¸¬ã€‚
5) æ ¼å¼é™å®šç‚º Markdownã€‚
"""

USER_SUMMARY_PROMPT_ZH = """ä»¥ä¸‹æ˜¯å¤šä»½æ–‡ä»¶å…§å®¹ï¼Œè«‹ç”¢å‡ºç¸½çµï¼š
{documents}

è«‹è¼¸å‡ºï¼š
- æ–‡ä»¶æ¸…å–®èˆ‡å°æ‡‰ç”¨é€”
- å„æ–‡ä»¶æ ¸å¿ƒè³‡è¨Šï¼ˆåç¨±ã€æ©Ÿæ§‹ã€åœ°å€ã€æ—¥æœŸã€ç·¨è™Ÿï¼‰
- å§”è¨—è£½é€ é—œè¯èˆ‡è²¬ä»»åˆ†å·¥
- å¯èƒ½é¢¨éšªæˆ–ç¼ºæ¼é …
- ä¸€æ®µçµå°¾çš„æ•´é«”è©•ä¼°

åœ¨é—œéµè©ï¼ˆå¦‚å…¬å¸åã€åœ°å€ã€å“é …åˆ†é¡ç´šåˆ¥ã€å¥‘ç´„é—œéµæ¢æ¬¾ã€æ—¥æœŸã€ç·¨è™Ÿç­‰ï¼‰å¤–å±¤åŠ ä¸Š <span style="color: coral">... </span>ã€‚
"""

SYSTEM_EXTRACTION_PROMPT_ZH = """ä½ æ˜¯ä¸€ä½çµæ§‹åŒ–è³‡æ–™æŠ½å–å¼•æ“ï¼Œç²¾é€šå°ç£é†«ç™‚å™¨æå§”è¨—è£½é€ ç›¸é—œæ–‡ä»¶ã€‚
è¦æ±‚ï¼š
- åƒ…å¾çµ¦å®šæ–‡ä»¶æŠ½å–è³‡è¨Šï¼Œè¼¸å‡º JSONï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼Œç¬¦åˆæŒ‡å®š schemaã€‚
- æœªå‡ºç¾çš„æ¬„ä½ä»¥ç©ºå­—ä¸² "" å¡«å…¥ï¼Œä¸è‡†æ¸¬ã€‚
- æ‰€æœ‰æ¬„ä½ä½¿ç”¨ç¹é«”ä¸­æ–‡èˆ‡åŸæ–‡å¿ å¯¦è¡¨è¿°ã€‚
- ä¸è¦åŒ…å«å¤šé¤˜æ–‡æœ¬ã€‚
"""

USER_EXTRACTION_PROMPT_ZH = """æ–‡ä»¶å¦‚ä¸‹ï¼Œä»¥ç¹é«”ä¸­æ–‡æŠ½å–ä»¥ä¸‹æ¬„ä½ï¼š
- å§”è¨—è€…åç¨±
- å§”è¨—è€…åœ°å€
- å—è¨—è€…åç¨±
- å—è¨—è€…åœ°å€
- å§”è¨—è£½é€ ä¹‹åˆæ„
- å§”è¨—è£½é€ ä¹‹é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …
- å§”è¨—è£½é€ ä¹‹è£½ç¨‹ï¼ˆä¾‹å¦‚ï¼šå…¨éƒ¨è£½ç¨‹å§”è¨—è£½é€ ï¼‰
- æ¬Šåˆ©ç¾©å‹™

æ–‡ä»¶å…§å®¹ï¼š
{document}

è«‹ä»¥ç´” JSON å›è¦†ã€‚"""

JSON_SCHEMA_EXTRACTION = {
    "type": "object",
    "properties": {
        "å§”è¨—è€…åç¨±": {"type": "string"},
        "å§”è¨—è€…åœ°å€": {"type": "string"},
        "å—è¨—è€…åç¨±": {"type": "string"},
        "å—è¨—è€…åœ°å€": {"type": "string"},
        "å§”è¨—è£½é€ ä¹‹åˆæ„": {"type": "string"},
        "å§”è¨—è£½é€ ä¹‹é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …": {"type": "string"},
        "å§”è¨—è£½é€ ä¹‹è£½ç¨‹": {"type": "string"},
        "æ¬Šåˆ©ç¾©å‹™": {"type": "string"},
    },
    "required": [
        "å§”è¨—è€…åç¨±",
        "å§”è¨—è€…åœ°å€",
        "å—è¨—è€…åç¨±",
        "å—è¨—è€…åœ°å€",
        "å§”è¨—è£½é€ ä¹‹åˆæ„",
        "å§”è¨—è£½é€ ä¹‹é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …",
        "å§”è¨—è£½é€ ä¹‹è£½ç¨‹",
        "æ¬Šåˆ©ç¾©å‹™"
    ],
    "additionalProperties": False
}
```

utils.py
--------------------------------
```python
from typing import Dict, Any, List
import pandas as pd
import plotly.express as px

def render_status_badge(label: str, status: str) -> str:
    cls = "badge-info"
    if status == "ok":
        cls = "badge-ok"
    elif status == "warn":
        cls = "badge-warn"
    elif status == "err":
        cls = "badge-err"
    return f'<span class="badge {cls}">{label}</span>'

def coralize_keywords(text: str) -> str:
    # If model already applied coral spans, keep them; otherwise a light heuristic to wrap obvious entities.
    # Here we just return as-is to avoid double-marking; rely on prompt to produce spans.
    return text

def to_markdown_table_zh(data: Dict[str, Any]) -> str:
    cols = list(data.keys())
    vals = [str(data.get(k, "")) for k in cols]
    # Build a simple Markdown table
    header = "| " + " | ".join(cols) + " |"
    sep = "| " + " | ".join([":---" for _ in cols]) + " |"
    row = "| " + " | ".join(vals) + " |"
    return "\n".join([header, sep, row])

def ensure_lang_zh(text: str) -> str:
    # Future: implement language detection/conversion if needed
    return text

def gen_dashboard_charts(metrics: Dict[str, Any]) -> List[Any]:
    charts = []
    # Pie chart for OCR methods
    ocr_counts = metrics.get("ocr_method_counts", {"local": 0, "llm": 0})
    pie_df = pd.DataFrame({
        "æ–¹æ³•": ["æœ¬åœ° OCR", "LLM OCR"],
        "æ¬¡æ•¸": [ocr_counts.get("local", 0), ocr_counts.get("llm", 0)]
    })
    charts.append(px.pie(pie_df, names="æ–¹æ³•", values="æ¬¡æ•¸", title="OCR æ–¹æ³•ä½¿ç”¨æ¯”ä¾‹"))

    # Bar for provider usage
    prov = metrics.get("provider_usage", {})
    if prov:
        bar_df = pd.DataFrame({"ä¾›æ‡‰å•†": list(prov.keys()), "æ¬¡æ•¸": list(prov.values())})
        charts.append(px.bar(bar_df, x="ä¾›æ‡‰å•†", y="æ¬¡æ•¸", title="ä¾›æ‡‰å•†ä½¿ç”¨æ¬¡æ•¸"))

    # Line for pages processed over time - if actions tracked
    # Here we simulate from total pages only
    pages = metrics.get("pages_processed", 0)
    line_df = pd.DataFrame({"åºåˆ—": list(range(1, (pages or 1)+1)), "é ç´¯è¨ˆ": list(range(1, (pages or 1)+1))})
    charts.append(px.line(line_df, x="åºåˆ—", y="é ç´¯è¨ˆ", title="é é¢è™•ç†ç´¯è¨ˆ"))
    return charts
```

agents.yaml (sample)
--------------------------------
```yaml
agents:
  - name: Summarizer-Refiner
    provider: Gemini
    model: gemini-2.5-flash
    parameters:
      temperature: 0.4
      max_tokens: 1400
    system_prompt: |
      ä½ æ˜¯æ‘˜è¦å„ªåŒ–å°ˆå®¶ã€‚è«‹åœ¨ä¸æ”¹è®Šæ—¢æœ‰äº‹å¯¦çš„å‰æä¸‹ï¼Œè®“æ‘˜è¦æ›´æ¸…æ™°ä¸”æ¢åˆ—æ›´ä¸€è‡´ï¼Œä¿ç•™ <span style="color: coral">...</span> æ¨™ç¤ºã€‚
    user_prompt: |
      è«‹å„ªåŒ–ä»¥ä¸‹æ‘˜è¦ä¸¦ç¶­æŒç¹é«”ä¸­æ–‡ï¼š
      ---
      {summary_md}
      ---
      è¦æ±‚ï¼š
      - æ¢åˆ—åˆ†ç¯€ä¸€è‡´
      - é—œéµè©æ¨™ç¤ºä¿æŒ
      - ç¼ºæ¼èˆ‡é¢¨éšªå¦åˆ—

  - name: Consistency-Checker
    provider: OpenAAI
    model: gpt-4.1-mini
    parameters:
      temperature: 0.1
      max_tokens: 1000
    system_prompt: |
      ä½ æ˜¯ä¸€è‡´æ€§ç¨½æ ¸åŠ©æ‰‹ï¼Œæ‰¾å‡ºè·¨æ–‡ä»¶çš„çŸ›ç›¾ã€ç¼ºæ¼æˆ–ä¸ä¸€è‡´ä¹‹è™•ã€‚
    user_prompt: |
      ä¾ä¸‹åˆ—æ–‡ä»¶èˆ‡æ‘˜è¦ï¼ŒæŒ‡å‡ºä¸ä¸€è‡´æˆ–éœ€è¦è£œä»¶çš„é …ç›®ï¼Œä»¥ç¹é«”ä¸­æ–‡æ¢åˆ—ï¼š
      [æ–‡ä»¶é¡å‹èˆ‡åç¨±]ï¼š
      {doc_names}
      [æ‘˜è¦]ï¼š
      {summary_md}

  - name: Field-Completer
    provider: Grok
    model: grok-4-fast-reasoning
    parameters:
      temperature: 0.2
      max_tokens: 1200
    system_prompt: |
      ä½ æ˜¯è³‡æ–™æ¬„ä½è£œé½ŠåŠ©æ‰‹ï¼Œåƒ…åœ¨æ–‡ä»¶æœ‰è³‡è¨Šæ™‚å¡«å…¥ï¼Œç„¡å‰‡ä»¥ç©ºå­—ä¸²è¡¨ç¤ºã€‚
    user_prompt: |
      åƒè€ƒä¸‹åˆ—æŠ½å– JSONèˆ‡æ‘˜è¦ï¼Œè«‹å»ºè­°æ˜¯å¦æœ‰æ¬„ä½å¯è£œå……æˆ–éœ€è¦æ¨™è¨»ç©ºç™½ï¼š
      [æŠ½å–JSON]ï¼š
      {extraction_json}
      [æ‘˜è¦]ï¼š
      {summary_md}
```

requirements.txt
--------------------------------
```
streamlit==1.39.0
PyYAML==6.0.2
google-generativeai==0.8.3
requests==2.32.3
pdfplumber==0.11.4
pdf2image==1.17.0
pytesseract==0.3.13
pillow==10.4.0
plotly==5.24.1
pandas==2.2.3
xai-sdk==0.2.7
```

Notes
- For LLM OCR, Gemini (gemini-2.5-flash) and OpenAAI (gpt-4o-mini assumed OpenAI-compatible) are supported for vision. Grok vision is attempted per sample but may require image URLs depending on xai_sdk version.
- If pdf2image requires poppler in your Space, add a system package in the Space build or use local OCR fallback via pdfplumber/pytesseract page images.
- API keys are taken from environment when set; otherwise masked input fields are provided. Keys are never printed.

Advanced Prompting Highlights
- Summarization prompt instructs coral keyword highlighting via <span style="color: coral">...</span>.
- Extraction prompt uses Traditional Chinese and strict JSON schema with graceful repair fallback.
- Agents allow further refinement, consistency checks, and field completion across providers.

Grok API usage (sample-integrated)
- GrokProvider.chat uses the provided xai_sdk pattern:
  client = Client(api_key=os.getenv("XAI_API_KEY"), timeout=3600)
  chat = client.chat.create(model="grok-4")
  chat.append(system(...)); chat.append(user(...)); response = chat.sample()

20 follow-up questions
1) Which provider and default models would you like preselected for each step (OCR-LLM, summarization, extraction, agents)?
2) Do you want us to automatically detect and recommend the best OCR path (digital vs scanned) per page instead of manual page selection?
3) Should the system attempt hybrid OCR (extract text via pdfplumber and only OCR pages with low text density)?
4) For LLM OCR, do you prefer chunking per page or batching multiple pages per request to optimize cost/time?
5) What maximum PDF size and page count should be allowed, and should there be a streaming preview for very large files?
6) Would you like keyword lists (e.g., company names, addresses, product classes) to be user-configurable to reinforce coral highlighting?
7) Should we add NER-based post-processing to auto-coralize keywords even if the LLM forgets the span styling?
8) For JSON validation, do you want strict schema enforcement with automatic re-ask to the model if invalid, up to N retries?
9) Should we include a human-in-the-loop review step for the JSON before finalizing and enabling download?
10) Do you need cross-document reconciliation (e.g., ensure å§”è¨—è€…/å—è¨—è€…åç¨±èˆ‡åœ°å€åœ¨å¤šæ–‡ä»¶é–“ä¸€è‡´ï¼Œå¦å‰‡æç¤ºå·®ç•°)?
11) Would you like a template-based exporter (CSV, Excel) in addition to JSON and Markdown table?
12) Should the agentsâ€™ outputs be versioned with a history panel and diff viewer to compare runs?
13) Do you want role-based access control or simple login protection for the Space?
14) Should the dashboard include cost estimates and token usage per provider if available?
15) Do you want a â€œone-click pipelineâ€ button to run OCR â†’ Summary â†’ Extraction â†’ Agents automatically?
16) For Grok vision OCR, would you like us to add optional temporary image hosting to support image URLs if the SDK requires it?
17) Should we add a redaction feature to mask sensitive fields (e.g., certificate numbers, addresses) before displaying or exporting?
18) Would you like multi-language support for UI (e.g., switch between ç¹ä¸­/English) while keeping outputs in Traditional Chinese?
19) Are there any additional fields you want in the extraction schema, such as æœ‰æ•ˆæœŸé–“ã€ç°½ç½²æ—¥æœŸã€è¯çµ¡äººã€çµ±ä¸€ç·¨è™Ÿ?
20) Should the system allow creating custom agents from the UI and saving back to agents.yaml for future runs?
